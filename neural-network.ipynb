{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff10206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from utils import load_house_data, plot_housing_prices, animate_neural_network\n",
    "\n",
    "sizes, prices, labels = load_house_data('data/housing_prices.txt')\n",
    "\n",
    "# As before, we load the housing data and scale it\n",
    "size_scaled = (sizes - np.mean(sizes)) / np.std(sizes)\n",
    "price_scaled = (prices - np.mean(prices)) / np.std(prices)\n",
    "\n",
    "# Create feature list: [size, price, size^2, price^2]\n",
    "features = [\n",
    "    size_scaled,      # Feature 0: size\n",
    "    price_scaled,     # Feature 1: price\n",
    "    size_scaled ** 2, # Feature 2: size^2\n",
    "    price_scaled ** 2 # Feature 3: price^2\n",
    "]\n",
    "\n",
    "# Convert to numpy array for easier computation\n",
    "feature_matrix = np.array(features).T  # Shape: (n_samples, n_features)\n",
    "feature_names = ['size', 'price', 'size²', 'price²']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f317752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    ####### YOUR CODE HERE #######\n",
    "    s = ...\n",
    "    ds = ...\n",
    "    ###### END OF YOUR CODE ######\n",
    "    return ds\n",
    "\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network for binary classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=4, hidden_size=6, output_size=1, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: number of input features\n",
    "        - hidden_size: number of neurons in hidden layer\n",
    "        - output_size: number of output neurons (1 for binary classification)\n",
    "        - activation: 'sigmoid' or 'relu' for hidden layer activation\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize weights randomly and and biases as zeros\n",
    "\n",
    "        ####### YOUR CODE HERE #######\n",
    "        # Hidden layer weights: (input_size, hidden_size)\n",
    "        self.W1 = ...\n",
    "        self.b1 = ...\n",
    "        \n",
    "        # Output layer weights: (hidden_size, output_size)\n",
    "        self.W2 = ...\n",
    "        self.b2 = ...\n",
    "        ###### END OF YOUR CODE ######\n",
    "        \n",
    "        # Choose activation function\n",
    "        self.activation_func = sigmoid\n",
    "        self.activation_derivative = sigmoid_derivative\n",
    "        \n",
    "        # Store training history\n",
    "        self.loss_history = []\n",
    "        self.weights_history = {'W1': [], 'b1': [], 'W2': [], 'b2': []}\n",
    "\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - A2: output predictions\n",
    "        - cache: intermediate values for backpropagation\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE #####\n",
    "        # Layer 1: Hidden layer\n",
    "        # Linear transformation: Z1 = X @ W1 + b1\n",
    "        Z1 = ...  # Shape: (n_samples, hidden_size)\n",
    "        \n",
    "        # Apply activation function\n",
    "        A1 = ...      # Shape: (n_samples, hidden_size)\n",
    "        \n",
    "        # Layer 2: Output layer\n",
    "        # Linear transformation: Z2 = A1 @ W2 + b2\n",
    "        Z2 = ... # Shape: (n_samples, output_size)\n",
    "        \n",
    "        # Apply sigmoid for binary classification\n",
    "        A2 = ...                   # Shape: (n_samples, output_size)\n",
    "        ######### END OF YOUR CODE ######\n",
    "        \n",
    "        # Store intermediate values for backpropagation\n",
    "        cache = {\n",
    "            'X': X,\n",
    "            'Z1': Z1,\n",
    "            'A1': A1,\n",
    "            'Z2': Z2,\n",
    "            'A2': A2\n",
    "        }\n",
    "        \n",
    "        return A2, cache\n",
    "    \n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate binary cross-entropy loss\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, cache, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to calculate gradients\n",
    "        \n",
    "        Parameters:\n",
    "        - cache: intermediate values from forward pass\n",
    "        - y_true: true labels\n",
    "        \n",
    "        Returns:\n",
    "        - gradients: dictionary containing all gradients\n",
    "        \"\"\"\n",
    "        # Retrieve cached values\n",
    "        ####### YOUR CODE HERE #######\n",
    "        X, Z1, A1, Z2, A2 = ...\n",
    "        m = X.shape[0]  # number of samples\n",
    "        \n",
    "        # Output layer gradients\n",
    "        # dL/dZ2 = A2 - y_true (derivative of sigmoid + cross-entropy)\n",
    "        # reshape y_true to match A2 shape via y_true.reshape(-1, 1)\n",
    "        dZ2 = ...  # Shape: (n_samples, 1)\n",
    "        \n",
    "        # dL/dW2 = A1.T @ dZ2 / m\n",
    "        dW2 = ...        # Shape: (hidden_size, 1)\n",
    "        \n",
    "        # dL/db2 = mean(dZ2) (over the first axis and keep dimensions)\n",
    "        db2 = ...  # Shape: (1, 1)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        # dL/dA1 = dZ2 @ W2.T\n",
    "        dA1 = ...      # Shape: (n_samples, hidden_size)\n",
    "        \n",
    "        # dL/dZ1 = dA1 * activation_derivative(Z1)\n",
    "        dZ1 = ...  # Shape: (n_samples, hidden_size)\n",
    "        \n",
    "        # dL/dW1 = X.T @ dZ1 / m\n",
    "        dW1 = ...        # Shape: (input_size, hidden_size)\n",
    "        \n",
    "        # dL/db1 = mean(dZ1) (over the first axis and keep dimensions)\n",
    "        db1 = ...  # Shape: (1, hidden_size)\n",
    "        ######### END OF YOUR CODE ######\n",
    "\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1': dW1,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        \"\"\"Update network parameters using gradients\"\"\"\n",
    "        self.W1 -= learning_rate * gradients['dW1']\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        y_pred, _ = self.forward_pass(X)\n",
    "        return y_pred.flatten()    \n",
    "\n",
    "    def train(self, X, y, learning_rate=0.1, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network using gradient descent\n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        - learning_rate: step size for parameter updates\n",
    "        - n_iterations: number of training iterations\n",
    "        Returns:\n",
    "        - loss history\n",
    "        - weights history\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        weights_history = []\n",
    "        for i in range(n_iterations):\n",
    "            # Forward propagation\n",
    "            y_pred, cache = self.forward_pass(X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(y, y_pred.flatten())\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_pass(cache, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients, learning_rate)\n",
    "            \n",
    "            # Store training history\n",
    "            loss_history.append(loss)\n",
    "            weights_history.append({\n",
    "                'W1': self.W1.copy(),\n",
    "                'b1': self.b1.copy(),\n",
    "                'W2': self.W2.copy(),\n",
    "                'b2': self.b2.copy()\n",
    "            })\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.6f}\")\n",
    "                print(f\"Sample gradients - dW1[0,0]: {gradients['dW1'][0,0]:.6f}, \"\n",
    "                      f\"dW2[0,0]: {gradients['dW2'][0,0]:.6f}\")\n",
    "\n",
    "        return loss_history, weights_history    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848f8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50, 1)\n",
      "First 5 predictions: [0.43428426 0.42540715 0.43782917 0.37974995 0.42039489]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to test if the foward pass works correctly\n",
    "nn = SimpleNeuralNetwork(input_size=4, hidden_size=6, output_size=1, activation='sigmoid')\n",
    "y_pred, cache = nn.forward_pass(feature_matrix)\n",
    "print(\"Predictions shape:\", y_pred.shape)\n",
    "print(\"First 5 predictions:\", y_pred[:5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "nn = SimpleNeuralNetwork(\n",
    "    input_size=feature_matrix.shape[1], \n",
    "    hidden_size=6, \n",
    "    output_size=1,\n",
    "    activation='sigmoid'\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network...\")\n",
    "loss_history, param_history = nn.train(feature_matrix, labels, learning_rate=0.5, n_iterations=500)\n",
    "\n",
    "# Make predictions\n",
    "predictions = nn.predict(feature_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean((predictions > 0.5) == labels)\n",
    "print(f\"\\nFinal Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "lambdas = []\n",
    "for params in param_history:\n",
    "    temp_nn = SimpleNeuralNetwork(\n",
    "        input_size=feature_matrix.shape[1],\n",
    "        hidden_size=6,\n",
    "        output_size=1,\n",
    "        activation='sigmoid'\n",
    "    )\n",
    "    temp_nn.W1 = params['W1']\n",
    "    temp_nn.b1 = params['b1']\n",
    "    temp_nn.W2 = params['W2']\n",
    "    temp_nn.b2 = params['b2']\n",
    "\n",
    "    lambdas.append(deepcopy(temp_nn.predict))\n",
    "\n",
    "# Create and display animation\n",
    "print(\"\\nCreating training animation...\")\n",
    "anim = animate_neural_network(\n",
    "    feature_matrix, lambdas, labels, loss_history,\n",
    "    feature_names, plot_every=10,\n",
    "    # save_path=\"output/neural_network_training.mp4\"\n",
    ")\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
