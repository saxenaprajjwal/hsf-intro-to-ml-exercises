{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104026,"databundleVersionId":12521872,"sourceType":"competition"},{"sourceId":242701488,"sourceType":"kernelVersion"},{"sourceId":244532811,"sourceType":"kernelVersion"},{"sourceType":"kernelVersion","sourceId":246258340}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# Jet Tagging with Boosting Decision Trees (BDT)\n\nThis notebook shows how to use XGBoost for jet classification between QCD and TT jets using particle-level features.\n\n## What is XGBoost?\nXGBoost (eXtreme Gradient Boosting) is a package for gradient boosted decision trees designed for speed and performance. It can handle large datasets and is perhaps the most used Python BDT implementation. \n\n## Why use BDTs for Jet Tagging?\n- Quick to train\n- More explainable than most other ML alternatives \n- Less prone to overfitting compared to deep learning methods\n- Despite being simple, BDTs often achieve good performance on complex tasks\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom utils import load_processed_data \nfrom plotting_utils import plot_confusion_matrix, plot_roc_curve","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T05:21:05.710228Z","execution_failed":"2025-06-19T05:23:23.733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Load and Prepare Data\n\nWe'll use particle-level features from the jet data. See kaggle for a detailed description of the data. ","metadata":{}},{"cell_type":"code","source":"# Load data\n# Create feature matrix and labels\n\nX_train, y_train, train_ids, X_val, y_val, val_ids, X_test, test_ids= load_processed_data()\n\nX_train.shape ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Train XGBoost Model\n\nXGBoost has several important hyperparameters:\n- `n_estimators`: How many trees to we use \n- `max_depth`: How deep is each tree - i.e. how many decisions does it make. Can you see a reason why we might want to set a max here? \n- `learning_rate`: How big is our gradient step \n- `objective`: Learning task and objective function - since we can do classification or regression it's important to select the right one here. ","metadata":{}},{"cell_type":"code","source":"# Initialize and train model\nmodel = xgb.XGBClassifier(\n    n_estimators=500,  # Number of boosting rounds\n    max_depth=10,      # Maximum tree depth\n    learning_rate=0.2, # Step size shrinkage\n    objective='binary:logistic',  # Binary classification\n    random_state=42\n)\n\n# Train the model\nmodel.fit(X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          verbose=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Evaluate Model\n\nLet's evaluate our model using:\n- Accuracy: Overall prediction accuracy\n- Confusion Matrix: This is common for classifications. How often did we classify or misclassify each category? It's common that some categories are harder than others. ","metadata":{}},{"cell_type":"code","source":"# Make predictions\n\n# this gives us probabilities for both categories - we only want for ttbar, so we select one column\n# with a binary classification, the probability for one category implies the other \ny_pred = model.predict_proba(X_val)[:, 1]\n\n# to test accuracy and confusion matrix, we need labels 0 and 1, so we set that based on a threshold\ndiscrete_pred = np.where(y_pred > 0.5, 1, 0)\n# Calculate accuracy\naccuracy = accuracy_score(y_val, np.where(y_pred > 0.5, 1, 0))\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot confusion matrix\nplot_confusion_matrix(y_val, discrete_pred)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_roc_curve(y_val, y_pred)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Feature Importance Analysis\n\nOne of the advantages of BDTs is that they provide feature importance scores, which help us understand which features are most important for the classification task. Beware that this doesn't translate directly to which are the best features in the data - only to what the model thinks. So if you have a bad model, your feature importance will be equally useless. Another model could also pick up on features that the BDT didn't, so it's only an indicator. ","metadata":{}},{"cell_type":"code","source":"# Plot feature importance\nimportance = model.feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': importance\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='feature', data=feature_importance)\nplt.title('Feature Importance')\nplt.tight_layout() \nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 - Make kaggle predictionsÂ¶\n","metadata":{}},{"cell_type":"code","source":"test_predictions = model.predict_proba(X_test)[:, 1]\nsolution = pd.DataFrame({'id':test_ids, 'label':test_predictions})\nsolution.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T05:23:23.736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}