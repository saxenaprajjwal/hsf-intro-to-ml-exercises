{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104026,"databundleVersionId":12521872,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":242701488,"sourceType":"kernelVersion"},{"sourceId":244532811,"sourceType":"kernelVersion"},{"sourceId":244576945,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jet Tagging with Transformers\n\nThis notebook shows how to use a Transformer architecture for jet classification between QCD and TT jets using anti-kt clustered particles. The transformer can capture complex interactions between particles through its self-attention mechanism.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom cluster_dataloader_utils import get_dataloaders\nfrom plotting_utils import plot_confusion_matrix, plot_training_history, plot_roc_curve\nfrom utils import load_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T10:15:42.618916Z","iopub.execute_input":"2025-06-20T10:15:42.619270Z","iopub.status.idle":"2025-06-20T10:15:42.625361Z","shell.execute_reply.started":"2025-06-20T10:15:42.619244Z","shell.execute_reply":"2025-06-20T10:15:42.624269Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"# 1 - Load data ","metadata":{}},{"cell_type":"markdown","source":"We are going to use the same data as we did in the deep set. Transformers usually excel at sequential data, but that is not quite what we have here. Instead, we are using the transformer to learn a relationship between the clusters we have found in the data, quite analogous to what we had in the graph neural nets. The actual jet tagging algorithms at CERN have been tried out with all the methods you have seen so far, and transformers seem to be the best performing. You can read more about the theoretical motivation in [this paper](https://arxiv.org/html/2406.08590v1) if you're interested. Do note that we are using simpler data than they are there. ","metadata":{}},{"cell_type":"code","source":"# Set data path and parameters\nbatch_size = 64\nR = 0.4  # Jet radius parameter\npt_min = 0.01  # Minimum pT threshold\nclusters_to_consider = 10\n# Get dataloaders\ntrain_loader, val_loader, test_loader = get_dataloaders(\n    batch_size=batch_size,\n    R=R,\n    pt_min=pt_min, \n    max_clusters=clusters_to_consider\n)\n\nprint(f\"Number of training batches: {len(train_loader)}\")\nprint(f\"Number of validation batches: {len(val_loader)}\")\nprint(f\"Number of test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T10:15:42.626955Z","iopub.execute_input":"2025-06-20T10:15:42.627840Z","iopub.status.idle":"2025-06-20T10:15:42.777624Z","shell.execute_reply.started":"2025-06-20T10:15:42.627804Z","shell.execute_reply":"2025-06-20T10:15:42.776675Z"}},"outputs":[{"name":"stdout","text":"Number of training batches: 55\nNumber of validation batches: 8\nNumber of test batches: 16\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"# 2- Define the model ","metadata":{}},{"cell_type":"markdown","source":"The model is based on the one found in [the paper mentioned earlier](https://arxiv.org/html/2406.08590v1). Note that it differs from many transformers in that there is no positional encoder. In an LLM for instance, the position of a word in a sentence is quite important, whereas here the position of the jets don't matter","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, input_dim=3, hidden_dim=64, num_heads=8, num_layers=2, output_dim=1):\n        super(Transformer, self).__init__()\n        \n        # Input projection\n        self.input_proj = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Transformer encoder layers\n        # this layer implements a transformer as laid out in the paper Attention Is All You Need.\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            batch_first=True,\n            dropout=0.1,\n            activation='gelu'  # Using GELU as in the paper\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Global pooling and classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n  \n    def forward(self, x):\n        # x shape: [batch_size, max_clusters, input_dim]\n        \n        # Project input\n        x = self.input_proj(x)  # Shape: [batch_size, max_clusters, hidden_dim]\n        \n        # Apply transformer\n        x = self.transformer(x)  # Shape: [batch_size, max_clusters, hidden_dim]\n        \n        # Global pooling (mean over clusters)\n        x = x.mean(dim=1)  # Shape: [batch_size, hidden_dim]\n        \n        # Classification head\n        x = self.classifier(x).squeeze()  # Shape: [batch_size, output_dim]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T10:15:42.778771Z","iopub.execute_input":"2025-06-20T10:15:42.779129Z","iopub.status.idle":"2025-06-20T10:15:42.788253Z","shell.execute_reply.started":"2025-06-20T10:15:42.779100Z","shell.execute_reply":"2025-06-20T10:15:42.787285Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# 3 - Train the model ","metadata":{}},{"cell_type":"markdown","source":"If you looked at the deep set notebook, you might notice that this is the exact same training loop. If you write good code, you can make it into functions and reuse it! ","metadata":{}},{"cell_type":"code","source":"# Create datasets\n# Initialize model and training components\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Transformer().to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-5,weight_decay=0.01)\n\n# optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n# from transformers import get_scheduler\n# lr_scheduler = get_scheduler(\n#     \"linear\",\n#     optimizer=optimizer,\n#     num_warmup_steps=1000,\n#     num_training_steps=total_steps  # define total_steps appropriately\n# )\n\nhistory = {\n    'loss': [],\n    'val_loss': [],\n    'accuracy': [],\n    'val_accuracy': []\n}\n\n# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    \n    for clusters, labels in train_loader:\n        clusters, labels = clusters.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(clusters)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        \n        # Calculate accuracy\n        preds = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += preds.eq(labels).sum().item()\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for clusters, labels in val_loader:\n            clusters, labels = clusters.to(device), labels.to(device)\n            outputs = model(clusters)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            # Calculate accuracy\n            preds = (outputs > 0.5).float()\n            val_total += labels.size(0)\n            val_correct += preds.eq(labels).sum().item()\n    \n    # Calculate epoch metrics\n    train_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n    train_acc = 100. * correct / total\n    val_acc = 100. * val_correct / val_total\n    \n    # Append to history\n    history['loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['accuracy'].append(train_acc)\n    history['val_accuracy'].append(val_acc)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T10:15:42.789860Z","iopub.execute_input":"2025-06-20T10:15:42.790259Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100:\nTrain Loss: 0.6697, Train Acc: 65.17%\nVal Loss: 0.5985, Val Acc: 66.33%\nEpoch 2/100:\nTrain Loss: 0.6295, Train Acc: 65.85%\nVal Loss: 0.5606, Val Acc: 66.33%\nEpoch 3/100:\nTrain Loss: 0.6001, Train Acc: 66.02%\nVal Loss: 0.5183, Val Acc: 66.33%\nEpoch 4/100:\nTrain Loss: 0.5717, Train Acc: 66.70%\nVal Loss: 0.4762, Val Acc: 67.13%\nEpoch 5/100:\nTrain Loss: 0.5426, Train Acc: 68.66%\nVal Loss: 0.4434, Val Acc: 73.90%\nEpoch 6/100:\nTrain Loss: 0.5247, Train Acc: 70.43%\nVal Loss: 0.4152, Val Acc: 79.88%\nEpoch 7/100:\nTrain Loss: 0.5038, Train Acc: 72.24%\nVal Loss: 0.3999, Val Acc: 82.27%\nEpoch 8/100:\nTrain Loss: 0.4886, Train Acc: 74.69%\nVal Loss: 0.3887, Val Acc: 84.06%\nEpoch 9/100:\nTrain Loss: 0.4816, Train Acc: 76.02%\nVal Loss: 0.3804, Val Acc: 83.86%\nEpoch 10/100:\nTrain Loss: 0.4650, Train Acc: 76.51%\nVal Loss: 0.3763, Val Acc: 84.06%\nEpoch 11/100:\nTrain Loss: 0.4645, Train Acc: 77.19%\nVal Loss: 0.3700, Val Acc: 84.06%\nEpoch 12/100:\nTrain Loss: 0.4555, Train Acc: 78.38%\nVal Loss: 0.3666, Val Acc: 84.26%\nEpoch 13/100:\nTrain Loss: 0.4571, Train Acc: 77.44%\nVal Loss: 0.3642, Val Acc: 85.26%\nEpoch 14/100:\nTrain Loss: 0.4458, Train Acc: 79.03%\nVal Loss: 0.3618, Val Acc: 84.66%\nEpoch 15/100:\nTrain Loss: 0.4413, Train Acc: 79.63%\nVal Loss: 0.3602, Val Acc: 84.66%\nEpoch 16/100:\nTrain Loss: 0.4397, Train Acc: 79.97%\nVal Loss: 0.3577, Val Acc: 85.06%\nEpoch 17/100:\nTrain Loss: 0.4346, Train Acc: 80.45%\nVal Loss: 0.3565, Val Acc: 84.46%\nEpoch 18/100:\nTrain Loss: 0.4418, Train Acc: 78.98%\nVal Loss: 0.3571, Val Acc: 85.26%\nEpoch 19/100:\nTrain Loss: 0.4340, Train Acc: 80.34%\nVal Loss: 0.3530, Val Acc: 85.26%\nEpoch 20/100:\nTrain Loss: 0.4318, Train Acc: 80.17%\nVal Loss: 0.3522, Val Acc: 85.26%\nEpoch 21/100:\nTrain Loss: 0.4300, Train Acc: 80.11%\nVal Loss: 0.3496, Val Acc: 85.06%\nEpoch 22/100:\nTrain Loss: 0.4311, Train Acc: 80.40%\nVal Loss: 0.3484, Val Acc: 84.86%\nEpoch 23/100:\nTrain Loss: 0.4285, Train Acc: 80.77%\nVal Loss: 0.3474, Val Acc: 85.06%\nEpoch 24/100:\nTrain Loss: 0.4326, Train Acc: 80.54%\nVal Loss: 0.3455, Val Acc: 85.26%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plot_training_history(history, metrics=['loss', 'accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 - Evaluating performance","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for clusters, labels in val_loader:  # Changed 'masks' to 'mask' to match dataset output\n        clusters, labels = clusters.to(device), labels.to(device)  # Move mask to device too\n        outputs = torch.sigmoid(model(clusters))  #Our output was logits, so we do sigmoid to get the probabilities\n        \n        all_preds.extend(outputs.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nall_preds_discrete = np.where(np.array(all_preds) > 0.5, 1, 0)\n# Calculate accuracy\naccuracy = np.mean(np.array(all_labels) == np.array(all_preds_discrete))\nprint(f'Test Accuracy: {accuracy:.4f}')\n\n# Plot confusion matrix\nplot_confusion_matrix(all_labels, all_preds_discrete)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_roc_curve(all_labels, all_preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 - Make predictions","metadata":{}},{"cell_type":"code","source":"y_pred_test = []\nfor data in test_loader:\n    data = data[0]\n    with torch.no_grad():\n        output = model(data.to(device))\n        # could you change the prediction threshold? Would that make it better?\n        prediction = torch.sigmoid(output).numpy()\n        y_pred_test.extend(prediction)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# this is a very lazy way to get the test_ids, the data hasn't been shuffled,so they'll be in \n# the same order. In a real example, we'd change the dataloader to also give our test_ids\nX_train, y_train, train_ids, X_val_, y_val, val_ids, X_test, test_ids = load_images()\n\n\nsolution = pd.DataFrame({'id':test_ids, 'label':y_pred_test})\nsolution.to_csv('solution.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}